{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.torch_utils import select_device\n",
    "from utils.general import check_img_size, non_max_suppression, xywh2xyxy, xyxy2xywh\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_PATH  = 'weights/yolov5s_thalas.pt'\n",
    "IMG_SIZE    = 640\n",
    "SAVE_DIR    = 'output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread('2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_target(output):\n",
    "    # Convert model output to target format [batch_id, class_id, x, y, w, h, conf]\n",
    "    targets = []\n",
    "    for i, o in enumerate(output):\n",
    "        for *box, conf, cls in o.cpu().numpy():\n",
    "            targets.append([i, cls, *list(*xyxy2xywh(np.array(box)[None])), conf])\n",
    "    return np.array(targets)\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "#     color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "\n",
    "    print('img: ', img.shape)\n",
    "\n",
    "    cv2.rectangle(img, c1, c2, color, tl)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def plot_image(image, target, output_dir, names):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().float().numpy()\n",
    "    if isinstance(target, torch.Tensor):\n",
    "        target = target.cpu().numpy()\n",
    "    # un-normalise\n",
    "    if np.max(image[0]) <= 1:\n",
    "        image *= 255\n",
    "    tl = 3  # line thickness\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    bs, _, h, w = image.shape\n",
    "     # Check if we should resize\n",
    "    scale_factor = IMG_SIZE / max(h, w)\n",
    "    image = image[0].transpose(1, 2, 0)\n",
    "    \n",
    "    if scale_factor < 1:\n",
    "        h = math.ceil(scale_factor * h)\n",
    "        w = math.ceil(scale_factor * w)\n",
    "        image = cv2.resize(image, (w, h))\n",
    "    if len(target) > 0:\n",
    "        boxes = xywh2xyxy(target[:, 2:6]).T\n",
    "        classes = target[:, 1]\n",
    "        labels = target.shape[1] == 6  # labels if no conf column\n",
    "        conf = None if labels else target[:, 6]  # check for confidence presence (label vs pred)\n",
    "        if boxes.shape[1]:\n",
    "            if boxes.max() <= 1.01:  # if normalized with tolerance 0.01\n",
    "                boxes[[0, 2]] *= w  # scale to pixels\n",
    "                boxes[[1, 3]] *= h\n",
    "            elif scale_factor < 1:  # absolute coords need scale if image scales\n",
    "                boxes *= scale_factor\n",
    "        for j, box in enumerate(boxes.T):\n",
    "            cls = int(classes[j])\n",
    "            cls = names[cls] if names else cls\n",
    "            if labels or conf[j] > 0.25:  # 0.25 conf thresh\n",
    "                label = '%s' % cls if labels else '%s %.1f' % (cls, conf[j])\n",
    "                plot_one_box(box, image, label=label, color=(255, 0, 0), line_thickness=tl)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(str(output_dir), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    \n",
    "    return img, ratio, (dw, dh)\n",
    "\n",
    "\n",
    "def preprocess(image, img_size = IMG_SIZE, stride = 32, pad = 0.5):\n",
    "    h0, w0 = image.shape[:2]  # orig hw    \n",
    "    r = IMG_SIZE / max(h0, w0)  # resize image to img_size\n",
    "    if r != 1:  # always resize down, only resize up if training with augmentation\n",
    "        interp = cv2.INTER_AREA if r < 1 else cv2.INTER_LINEAR\n",
    "        image = cv2.resize(image, (int(w0 * r), int(h0 * r)), interpolation=interp)\n",
    "    \n",
    "    ar = w0/h0\n",
    "    if ar < 1:\n",
    "        shapes = [ar, 1]\n",
    "    elif ar > 1:\n",
    "        shapes = [1, 1 / ar]\n",
    "        \n",
    "    img_shape = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int) * stride\n",
    "\n",
    "    img = letterbox(image, img_shape, stride=stride)[0]\n",
    "    \n",
    "    # Convert\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "    return img\n",
    "\n",
    "def inference(image, name):\n",
    "    # Select device\n",
    "    device = select_device(batch_size=1)\n",
    "    \n",
    "    # Load model\n",
    "    model = attempt_load(MODEL_PATH, map_location=device)\n",
    "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "    imgsz = check_img_size(IMG_SIZE, s=gs)  # check img_size\n",
    "    # Half\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "    if half:\n",
    "        model.half()  \n",
    "    \n",
    "    # Configure\n",
    "    model.eval()\n",
    "    \n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    \n",
    "    classes = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "    \n",
    "    # preprocess image\n",
    "    image = preprocess(image)\n",
    "    image = torch.from_numpy(np.expand_dims(image, axis=0))\n",
    "    image = image.to(device, non_blocking=True)\n",
    "    image = image.half() if half else image.float()  # uint8 to fp16/32\n",
    "    image /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "#     nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run model\n",
    "        out, train_out = model(image, augment=False)  # inference and training outputs\n",
    "        out = non_max_suppression(out, conf_thres=0.001, iou_thres=0.5, multi_label=True)\n",
    "        \n",
    "        \n",
    "    targets = output_to_target(out)\n",
    "    \n",
    "    # draw boxes\n",
    "    plot_image(image, targets, name, classes)\n",
    "    \n",
    "    print(\"************ DONE **************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path ='2.png'\n",
    "image = cv2.imread(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path ='2.png'\n",
    "image = cv2.imread(img_path)\n",
    "\n",
    "# Select device\n",
    "device = select_device(batch_size=1)\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(MODEL_PATH, map_location=device)\n",
    "gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "imgsz = check_img_size(IMG_SIZE, s=gs)  # check img_size\n",
    "# Half\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "if half:\n",
    "    model.half()  \n",
    "\n",
    "# Configure\n",
    "model.eval()\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "classes = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "\n",
    "# preprocess image\n",
    "image = preprocess(image)\n",
    "image = torch.from_numpy(np.expand_dims(image, axis=0))\n",
    "image = image.to(device, non_blocking=True)\n",
    "image = image.half() if half else image.float()  # uint8 to fp16/32\n",
    "image /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "#     nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Run model\n",
    "    out, train_out = model(image, augment=False)  # inference and training outputs\n",
    "    out = non_max_suppression(out, conf_thres=0.001, iou_thres=0.5, multi_label=True)\n",
    "\n",
    "\n",
    "target = output_to_target(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(image, torch.Tensor):\n",
    "    image = image.cpu().float().numpy()\n",
    "if isinstance(target, torch.Tensor):\n",
    "    target = target.cpu().numpy()\n",
    "# un-normalise\n",
    "if np.max(image[0]) <= 1:\n",
    "    image *= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread('2.png')\n",
    "box = np.array([10,10 , 200, 200])\n",
    "plot_one_box(box, im, label='label', color=(255, 0, 0), line_thickness=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.shape, type(im), np.unique(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "im2 = cv2.imread('2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2.shape, type(im2), np.unique(im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
